{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plt.rc('figure', figsize=(8, 4), dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set up Weights & Biases tracking\n",
    "\n",
    "To make this interesting, we're tracking everyone's progress via W&B. Don't worry \n",
    "Please replace `<Your name here>` by your own name (or a nickname).\n",
    "\n",
    "If you just want to play around without uploading your runs, set `offline` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for identifying runs on the W&B dashboard\n",
    "# name = \"<Your name here>\"\n",
    "name = \"<Your name here>\"\n",
    "\n",
    "# Whether or not to upload runs to W&B\n",
    "upload = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"<Your name here>\":\n",
    "  raise Exception(\"RTFM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/full.csv\")\n",
    "all_feature_names = df.iloc[:, 1:].columns.to_list()\n",
    "\n",
    "X = df.iloc[:, 1:].to_numpy()\n",
    "y = df.iloc[:, 0].to_numpy()\n",
    "feature_names = df.iloc[:, 1:].columns.to_numpy()    \n",
    "labels = [\"not buggy\", \"buggy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 70% train and 30% test subsets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our models\n",
    "\n",
    "The value of features can differ greatly between models, which is why we test across a number of common and easy-to-train ones. We also look at the effect of regularization, as this can also affect the impact of feature selection.\n",
    "\n",
    "* Decision Tree\n",
    "* Random Forrest\n",
    "* Logistic Regression\n",
    "* Logistic Regression (with L1 regularization)\n",
    "* Logistic Regression (with L2 regularization)\n",
    "* K Nearest Neighbors (with k=15)\n",
    "\n",
    "\n",
    "_There is no need to change anything here. These are just helper functions to quickly test our data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "        (DecisionTreeClassifier(), ),\n",
    "        (RandomForestClassifier(n_estimators=200, min_samples_leaf=10, max_depth=20, class_weight=\"balanced\"), ),\n",
    "        # (SVC(gamma=\"auto\", probability=True, class_weight=class_weight), ),\n",
    "        (LogisticRegression(max_iter=1000, class_weight=\"balanced\"), ),\n",
    "        (LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\"), \"_l1\"),\n",
    "        (LogisticRegression(max_iter=1000, penalty=\"l2\", class_weight=\"balanced\"), \"_l2\"),\n",
    "        (KNeighborsClassifier(n_neighbors=15), )\n",
    "    ]\n",
    "classifiers = [(c[0], c[0].__class__.__name__ + (c[1] if len(c) > 1 else \"\")) for c in classifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our testing code\n",
    "\n",
    "These functions enable us to easily train our set of classifiers using specific features. They also help us keep track of all experiments\n",
    "\n",
    "*There's no need to change anything here, either*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(clf, feature_indices):\n",
    "    if feature_indices is None or len(feature_indices) == 0:\n",
    "        feature_indices = list(range(X_train.shape[1]))\n",
    "    clf.fit(X_train[:, feature_indices], y_train)\n",
    "\n",
    "    # predicted = clf.predict(X_test)\n",
    "    y_probas = clf.predict_proba(X_test[:, feature_indices])\n",
    "    y_pred = y_probas.argmax(axis=1)\n",
    "\n",
    "    scores = {\n",
    "        f\"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "        f\"F1\": metrics.f1_score(y_test, y_pred),\n",
    "        f\"Precision\": metrics.precision_score(y_test, y_pred),\n",
    "        f\"Recall\": metrics.recall_score(y_test, y_pred),\n",
    "    }\n",
    "    return scores, y_probas\n",
    "\n",
    "\n",
    "def test_selection(feature_indices, compare=None, log=True, log_wandb=False):\n",
    "    scores = {}\n",
    "    table = []\n",
    "    for classifier, name in tqdm(classifiers, smoothing=0):\n",
    "        scores_ = test_classifier(classifier, feature_indices=feature_indices)[0]\n",
    "        scores[name] = scores_\n",
    "\n",
    "    # This has no place being one statement, but I just want it to be one\n",
    "    scores[\"Average\"] = dict(\n",
    "        zip(\n",
    "            next(iter(scores.values())).keys(),\n",
    "            np.mean(list(zip(*(x.values() for x in scores.values()))), axis=1),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for name, scores_ in scores.items():\n",
    "        row = [name]\n",
    "        row += [\n",
    "            f\"{s:0.4f}\" + (f\"{baseline[name][m]:0.4f}\" if compare else \"\")\n",
    "            for m, s in scores_.items()\n",
    "        ]\n",
    "\n",
    "        row = {\n",
    "            m: f\"{s:0.4f}\" + (f\" ({s - compare[name][m]:0.4f})\" if compare else \"\")\n",
    "            for m, s in scores_.items()\n",
    "        }\n",
    "        row = {\"Classifier\": name, **row}\n",
    "        table.append(row)\n",
    "\n",
    "        for metric, score in scores_.items():\n",
    "            rep = f\"{score:0.4f}\"\n",
    "            if compare:\n",
    "                rep += f\" ({compare[name][metric]:0.4f})\"\n",
    "\n",
    "    print(tabulate(table, headers=\"keys\"))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def test_selector(selector, run_suffix=None, wandb=True, **config_kwargs):\n",
    "    used_features = selector.get_support()\n",
    "    used_feature_names = selector.get_feature_names_out(feature_names)\n",
    "    print(\n",
    "        f\"Selected {sum(used_features)}/{len(feature_names)} features: \\n{used_feature_names}\"\n",
    "    )\n",
    "\n",
    "    scores = test_selection(used_features, compare=baseline)\n",
    "\n",
    "    if wandb:\n",
    "        log_scores(\n",
    "            scores,\n",
    "            used_feature_names,\n",
    "            selector.__class__.__name__,\n",
    "            run_suffix,\n",
    "            **config_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def log_scores(scores, used_features, method_name, method_suffix=None, **config_kwargs):\n",
    "    print(used_features)\n",
    "    if all([isinstance(x, int) for x in used_features]):\n",
    "        print(\"Detected features as string indices\")\n",
    "        used_feature_names = feature_names[used_features]\n",
    "    elif all([isinstance(x, str) for x in used_features]):\n",
    "        print(\"Detected features as string indices\")\n",
    "        used_feature_names = used_features\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Whoops, expected used_features to be a list of names or indices\"\n",
    "        )\n",
    "\n",
    "    used_features_tbl = {name: name in used_feature_names for name in feature_names}\n",
    "    time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    method = method_name + (\"_\" + method_suffix if method_suffix else \"\")\n",
    "    wandb.init(\n",
    "        project=\"sogeti-hackathon-feature-selection\",\n",
    "        entity=\"vincentbrouwers\",\n",
    "        name=\"-\".join([name, method, time]),\n",
    "        anonymous=\"allow\",\n",
    "        tags=dict(name=name),\n",
    "        mode=\"online\" if upload else \"offline\",\n",
    "        config=dict(\n",
    "            features=used_features_tbl,\n",
    "            name=name,\n",
    "            method=method,\n",
    "            method_name=method_name,\n",
    "            method_suffix=method_suffix,\n",
    "            **config_kwargs,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    wscores = {\n",
    "        f\"{clf}_{mtr}\": score\n",
    "        for clf, mtrs in scores.items()\n",
    "        for mtr, score in mtrs.items()\n",
    "    }\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"feature_count\": len(used_feature_names),\n",
    "            \"used_features\": wandb.Table(\n",
    "                columns=[\"feature\", \"used\"],\n",
    "                # Just temporary until I have an actual method of filtering features\n",
    "                data=list(used_features_tbl.items()),\n",
    "            ),\n",
    "            # \"roc\": wandb.plot.roc_curve(y_test, y_probas, labels),\n",
    "            # \"pr\": wandb.plot.pr_curve(y_test, y_probas, labels),\n",
    "            **wscores,\n",
    "        }\n",
    "    )\n",
    "    wandb.finish(quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = test_selection(None, compare=False)\n",
    "log_scores(baseline, feature_names, \"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance threshold\n",
    "\n",
    "A simple method to filter out superfluous features, is to remove ones with a low variance. The idea here is that these features offer relatively little information. \n",
    "\n",
    "*Note: Variance depends on the magnitude our values and our data is not normalized. It's not really possible to set 1 threshold for all features*\n",
    "\n",
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var = VarianceThreshold(threshold=0.3).fit(X_train)\n",
    "\n",
    "test_selector(var, threshold=var.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical corrolation threshold\n",
    "\n",
    "Another method of supervised feature filtering is to calculate corrolation statistics between each feature and the label(s). \n",
    "\n",
    "We use the chi-squared test to measure the label's (positive) dependence on features and select the best ones with various thresholding methods.\n",
    "\n",
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html<br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import chi2, r_regression, mutual_info_classif\n",
    "\n",
    "# Select (k=)10 most corrolated features \n",
    "selector = SelectKBest(chi2, k=10).fit(X_train, y_train)\n",
    "test_selector(selector, \"chi2\")\n",
    "\n",
    "# Select the features that hit the 50th percentile (median score or better)\n",
    "selector = SelectPercentile(chi2, percentile=50).fit(X_train, y_train)\n",
    "test_selector(selector, \"chi2\")\n",
    "\n",
    "# # Select (k=)10 most corrolated features \n",
    "# selector = SelectKBest(r_regression, k=10).fit(X_train, y_train)\n",
    "# test_selector(selector, \"r_regression\")\n",
    "\n",
    "# # Select the features that hit the 50th percentile (median score or better)\n",
    "# selector = SelectPercentile(r_regression, percentile=50).fit(X_train, y_train)\n",
    "# test_selector(selector, \"r_regression\")\n",
    "\n",
    "# # Select (k=)10 most corrolated features \n",
    "# selector = SelectKBest(mutual_info_classif, k=10).fit(X_train, y_train)\n",
    "# test_selector(selector, \"mutual_info_classif\")\n",
    "\n",
    "# # Select the features that hit the 50th percentile (median score or better)\n",
    "# selector = SelectPercentile(mutual_info_classif, percentile=50).fit(X_train, y_train)\n",
    "# test_selector(selector, \"mutual_info_classif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-specific feature importance\n",
    "\n",
    "Some models allow us to directly see the contribution of each feature. This allows us to easily remove the fields that our model extracts the least amount of information from. We always test the effect our selection methods on multiple model types. Does te type of reference model the model affect our testing models differently?\n",
    "\n",
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_estimators=200).fit(X_train, y_train)\n",
    "# plt.barh(feature_names, clf1.feature_importances_)\n",
    "# selector = SelectFromModel(clf1, threshold=\"1.5 * mean\", prefit=True)\n",
    "\n",
    "clf2 = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "plt.barh(feature_names, clf2.coef_.flatten())\n",
    "selector = SelectFromModel(clf2, threshold=\"1 * mean\", prefit=True)\n",
    "\n",
    "\n",
    "test_selector(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination (RFE)\n",
    "\n",
    "\n",
    "RFE works in a similar fashion as the previous method, namely that it uses a model's built-in feature significance values to filter out redundant features. Where RFE differs from the \"naive\" filtering aproach, is that it only removes one feature at a time, after which the entire model is retrained again. Removal of corrolated or inter-dependent features can change the distribution of significance of the remaining features, which this approach mitigates.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "A simple example of how the result of RFE differs from \"naive filtering\", is when two features are 100% corrolated. Each of them may be fairly meaningful on their own, though when they're both present, their contribution has to be shared between them. This thus gives them a lower feature importance. \n",
    "\n",
    "Let's say we have a model with the following feature importances:\n",
    "\n",
    "`a=15%`, `b=20%`, `c=50%`, `d=15%`, where `a`, `b` , and `c` are fully independent of eachother, but `d` is 100% corrolated to `a`. \n",
    "\n",
    "Na√Øvely removing the two least contributing features, would leave us with `b=35%` and `c=65%`. If we instead first eliminate `c` (`a` is equally valid), a new model might give us these importances:\n",
    "\n",
    "`a=30%`, `b=20%`, `c=50%`\n",
    "\n",
    "This time, `b` will be purged, leaving us with\n",
    "\n",
    "`a=40%`, `c=60%`\n",
    "\n",
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "selector = RFE(clf, n_features_to_select=5, step=1, verbose=1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "test_selector(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Feature Selection\n",
    "\n",
    "This Sequential Feature Selector adds (forward selection) or removes (backward selection) features until the desired amount of features is reached. At each stage, it produces candidate feature sets that include (forward) or exclude (backward) one feature compared to the previous stage. The candidate sets are scored by training new models on them and only the best scoring one is kept.\n",
    "\n",
    "Forwards and backwards do not have to yield the same feature sets, though none is necessarily better. Their performance can differ depending on the amount and size of models that need to be trained to reach the desired amount of features.\n",
    "\n",
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "direction = \"forward\"\n",
    "# direction = \"backward\"\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction=direction, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "test_selector(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own selection\n",
    "\n",
    "Now try it for yourself. Is there another method of feature selection you would like to try? Do yo think combining other methods might "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "    14,\n",
    "    15,\n",
    "    16,\n",
    "    17,\n",
    "    18,\n",
    "    19,\n",
    "    20,\n",
    "    21,\n",
    "    22,\n",
    "]\n",
    "\n",
    "scores = test_selection(used_features, compare=baseline)\n",
    "\n",
    "# Uncomment this as soon as you want to upload your run to Weights&Biases. \n",
    "# log_scores(scores, used_features, \"Custom\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "194a1ef71cbe383bb0e91fd20851e850791a3f40c244816884283bb052247dec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
